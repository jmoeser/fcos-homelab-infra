# ollama.container — Ollama LLM server
# Shares network namespace with ts-ollama (Tailscale sidecar).
# Accessible on the tailnet via HTTPS (TS serve → localhost:11434).
# OpenClaw reaches it directly at http://10.89.1.3:11434 via app-network.
#
# Memory: 10g ceiling leaves ~6g for OS + other containers.
# Models are stored in the ollama-models named volume (on SD card — keep models small).

[Unit]
Description=Ollama LLM server
After=ts-ollama.service
Requires=ts-ollama.service
StartLimitIntervalSec=300
StartLimitBurst=5

[Container]
ContainerName=ollama
Image=ollama/ollama:latest
AutoUpdate=registry

# Share network namespace with the Tailscale sidecar
Network=ts-ollama.container

Volume=ollama-models.volume:/root/.ollama:Z

PodmanArgs=--memory=10g

[Service]
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target default.target
